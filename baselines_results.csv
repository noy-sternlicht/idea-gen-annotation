example_id,annotator,context,gold,baseline,suggestion,k,sci_sense,novelty,specificity,interest
1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,noy.sternlicht@mail.huji.ac.il,"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",medical training,random,encoder-decoder model,1,Low,High,Med,Low
1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,noy.sternlicht@mail.huji.ac.il,"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",medical training,ours,"Inspired by this, we investigate the role of medical knowledge in disease diagnosis through doctor-patient interaction",1,Med,Med,High,Med
1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,noy.sternlicht@mail.huji.ac.il,"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",medical training,gpt-4o,clinical guidelines,1,High,Med,Med,High
1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,noy.sternlicht@mail.huji.ac.il,"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",medical training,sciIE,semantic high dimensional medical knowledge,1,Med,Low,Med,Low
1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,noy.sternlicht@mail.huji.ac.il,"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",medical training,mpnet_zero,enhance the capability of medical Multimodal Large Languange Models in understanding anatomical regions within entire medical scans,1,Med,Low,High,Low
1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,noy.sternlicht@mail.huji.ac.il,"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",recent advancements of large language models,random,the cumulative sum statistic,1,Low,High,Med,Low
1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,noy.sternlicht@mail.huji.ac.il,"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",recent advancements of large language models,ours,students' Intelligent Tutoring System performance metrics,1,Med,Low,High,Low
1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,noy.sternlicht@mail.huji.ac.il,"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",recent advancements of large language models,gpt-4o,intelligent tutoring systems,1,High,Med,Med,Low
1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,noy.sternlicht@mail.huji.ac.il,"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",recent advancements of large language models,sciIE,AI tutors,1,High,Med,Med,Low
1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,noy.sternlicht@mail.huji.ac.il,"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",recent advancements of large language models,mpnet_zero,build dialog tutoring models that scaffold students' problem-solving,1,High,Med,High,Low
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,random,Vision-Language foundation models (VL-models),1,Med,Med,High,Med
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,ours,a sequential decision-making problem,1,Med,Med,Med,Med
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,gpt-4o,dynamic neural networks,1,Med,Med,Med,Med
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,sciIE,few-shot capabilities of large language models,1,Med,Med,High,Med
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,mpnet_zero,recent success of Large Language Models,1,Med,Low,Med,Low
1-39276_43781121-205c-457f-8985-5aa9fc712ef0,noy.sternlicht@mail.huji.ac.il,"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",a curriculum learning process,random,a dynamic observation module,1,Low,Low,Low,Low
1-39276_43781121-205c-457f-8985-5aa9fc712ef0,noy.sternlicht@mail.huji.ac.il,"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",a curriculum learning process,ours,knowledge transferring in multi-task learning,1,Med,High,Med,Med
1-39276_43781121-205c-457f-8985-5aa9fc712ef0,noy.sternlicht@mail.huji.ac.il,"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",a curriculum learning process,gpt-4o,curriculum learning,1,High,Med,Med,Med
1-39276_43781121-205c-457f-8985-5aa9fc712ef0,noy.sternlicht@mail.huji.ac.il,"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",a curriculum learning process,sciIE,Large Language Model,1,High,Low,Med,Low
1-39276_43781121-205c-457f-8985-5aa9fc712ef0,noy.sternlicht@mail.huji.ac.il,"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",a curriculum learning process,mpnet_zero,the training processes of large language models,1,High,Low,Med,Low
1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,noy.sternlicht@mail.huji.ac.il,"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",a string sequence of characters,random,hyperdimensional vector computing algebras,1,Low,High,High,Low
1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,noy.sternlicht@mail.huji.ac.il,"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",a string sequence of characters,ours,a sequence of temporal graphs,1,Med,High,Med,High
1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,noy.sternlicht@mail.huji.ac.il,"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",a string sequence of characters,gpt-4o,time-series analysis,1,High,Med,Med,Low
1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,noy.sternlicht@mail.huji.ac.il,"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",a string sequence of characters,sciIE,learning analytics,1,Med,Low,Med,Low
1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,noy.sternlicht@mail.huji.ac.il,"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",a string sequence of characters,mpnet_zero,learning analytics,1,Med,Low,Med,Low
1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,noy.sternlicht@mail.huji.ac.il,"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",an explainable visual entailment task,random,a synthetic degradation pipeline,1,Low,High,Med,Low
1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,noy.sternlicht@mail.huji.ac.il,"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",an explainable visual entailment task,ours,a vision-language fusion problem,1,Med,Low,Med,Low
1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,noy.sternlicht@mail.huji.ac.il,"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",an explainable visual entailment task,gpt-4o,cognitive linguistics,1,Med,Med,Med,Med
1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,noy.sternlicht@mail.huji.ac.il,"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",an explainable visual entailment task,sciIE,fine-grained understanding of literal meaning,1,Med,Med,Med,Low
1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,noy.sternlicht@mail.huji.ac.il,"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",an explainable visual entailment task,mpnet_zero,visual analogical reasoning in large multimodal models,1,High,Med,High,Med
1-29581_8487266a-eb35-4090-9193-90e818d727f7,noy.sternlicht@mail.huji.ac.il,"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Adaptive LASSO regression model,random,an energy score,1,Low,High,Med,Low
1-29581_8487266a-eb35-4090-9193-90e818d727f7,noy.sternlicht@mail.huji.ac.il,"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Adaptive LASSO regression model,ours,the influence of label smoothing on model confidence and differential privacy,1,Med,Med,High,Med
1-29581_8487266a-eb35-4090-9193-90e818d727f7,noy.sternlicht@mail.huji.ac.il,"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Adaptive LASSO regression model,gpt-4o,zero-shot learning,1,Med,Med,Med,High
1-29581_8487266a-eb35-4090-9193-90e818d727f7,noy.sternlicht@mail.huji.ac.il,"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Adaptive LASSO regression model,sciIE,few-shot capabilities of large language models,1,Med,Med,Med,High
1-29581_8487266a-eb35-4090-9193-90e818d727f7,noy.sternlicht@mail.huji.ac.il,"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Adaptive LASSO regression model,mpnet_zero,existing calibration methods for large language models,1,High,Low,Med,Low
1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",a thread of execution,random,little human input,1,Low,High,Med,Low
1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",a thread of execution,ours,complex task-solving processes,1,High,High,Med,High
1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",a thread of execution,gpt-4o,cognitive psychology problem-solving strategies,1,High,High,Med,High
1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",a thread of execution,sciIE,Large Language Model,1,Low,Low,Med,Low
1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",a thread of execution,mpnet_zero,improve the performance of Large Language Models on complex reasoning and planning tasks,1,Low,Low,High,Low
1-16286_486004d5-f250-476c-8346-a56ced165a62,noy.sternlicht@mail.huji.ac.il,"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",text generation in natural language processing,random,3D hand poses at a high temporal resolution for fine-grained motion modeling,1,Low,High,High,Low
1-16286_486004d5-f250-476c-8346-a56ced165a62,noy.sternlicht@mail.huji.ac.il,"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",text generation in natural language processing,ours,medical visual question answering (Med-VQA),1,High,High,High,High
1-16286_486004d5-f250-476c-8346-a56ced165a62,noy.sternlicht@mail.huji.ac.il,"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",text generation in natural language processing,gpt-4o,self-supervised learning,1,Med,Med,Med,Med
1-16286_486004d5-f250-476c-8346-a56ced165a62,noy.sternlicht@mail.huji.ac.il,"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",text generation in natural language processing,sciIE,medical image classification,1,High,Low,Med,Low
1-16286_486004d5-f250-476c-8346-a56ced165a62,noy.sternlicht@mail.huji.ac.il,"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",text generation in natural language processing,mpnet_zero,multi-label image classification,1,High,Low,Med,Low
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,noy.sternlicht@mail.huji.ac.il,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",the rich interactions between language and decision-making in humans,random,visual tokens,1,Low,High,Med,Low
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,noy.sternlicht@mail.huji.ac.il,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",the rich interactions between language and decision-making in humans,ours,Humans can quickly learn new behaviors by leveraging background world knowledge,1,Med,Med,High,Med
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,noy.sternlicht@mail.huji.ac.il,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",the rich interactions between language and decision-making in humans,gpt-4o,cognitive neuroscience,1,Med,Med,Med,Med
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,noy.sternlicht@mail.huji.ac.il,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",the rich interactions between language and decision-making in humans,sciIE,AI agents,1,High,Low,Med,Low
1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,noy.sternlicht@mail.huji.ac.il,"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",the rich interactions between language and decision-making in humans,mpnet_zero,replicating human-like cognitive processes in AI,1,Med,Med,Med,Med
1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,random,the human review process,1,Low,Low,Low,Low
1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,ours,emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,1,High,High,High,High
1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,gpt-4o,human creative processes,1,High,Med,Med,Med
1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,sciIE,open language models,1,Med,Med,Med,Low
1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,mpnet_zero,assessing the level of creativity in large language models,1,Low,High,Med,Low
1-5936_93733f00-5c88-4125-947d-5061e595fcfd,noy.sternlicht@mail.huji.ac.il,"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",avant-garde literature,random,LLM-based logits,1,Low,High,Med,Low
1-5936_93733f00-5c88-4125-947d-5061e595fcfd,noy.sternlicht@mail.huji.ac.il,"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",avant-garde literature,ours,Flower and Hayes' cognitive process theory of writing,1,High,High,High,High
1-5936_93733f00-5c88-4125-947d-5061e595fcfd,noy.sternlicht@mail.huji.ac.il,"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",avant-garde literature,gpt-4o,literary history,1,Med,High,Med,Med
1-5936_93733f00-5c88-4125-947d-5061e595fcfd,noy.sternlicht@mail.huji.ac.il,"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",avant-garde literature,sciIE,writing systems,1,High,Low,Med,Low
1-5936_93733f00-5c88-4125-947d-5061e595fcfd,noy.sternlicht@mail.huji.ac.il,"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",avant-garde literature,mpnet_zero,Flower and Hayes' cognitive process theory of writing,1,High,High,High,High
1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,noy.sternlicht@mail.huji.ac.il,"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",a system of reinforcement learning from human feedback,random,image-based conditioning through cross-attention,1,Low,High,Med,Low
1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,noy.sternlicht@mail.huji.ac.il,"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",a system of reinforcement learning from human feedback,ours,a system of reinforcement learning from human feedback,1,High,Med,High,Med
1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,noy.sternlicht@mail.huji.ac.il,"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",a system of reinforcement learning from human feedback,gpt-4o,active learning techniques,1,High,Med,Med,Med
1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,noy.sternlicht@mail.huji.ac.il,"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",a system of reinforcement learning from human feedback,sciIE,human expert proofreading,1,Low,Low,Low,Low
1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,noy.sternlicht@mail.huji.ac.il,"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",a system of reinforcement learning from human feedback,mpnet_zero,the evaluation of generated text,1,Low,High,Med,Low
1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,noy.sternlicht@mail.huji.ac.il,"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",translating text into an Abstract Meaning Representation graph,random,machine learning behavior models,1,Med,Med,Med,Med
1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,noy.sternlicht@mail.huji.ac.il,"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",translating text into an Abstract Meaning Representation graph,ours,Miller's cognitive model of explanation,1,High,High,High,High
1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,noy.sternlicht@mail.huji.ac.il,"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",translating text into an Abstract Meaning Representation graph,gpt-4o,truth tables,1,Low,Low,Low,Low
1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,noy.sternlicht@mail.huji.ac.il,"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",translating text into an Abstract Meaning Representation graph,sciIE,natural language inference,1,Low,Low,Med,Low
1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,noy.sternlicht@mail.huji.ac.il,"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",translating text into an Abstract Meaning Representation graph,mpnet_zero,Natural Language Inference models,1,High,Low,Med,Low
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,noy.sternlicht@mail.huji.ac.il,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",a natural language inference task,random,a more robust and efficient control mechanism for autonomous manipulation in robot arms,1,Low,High,High,Low
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,noy.sternlicht@mail.huji.ac.il,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",a natural language inference task,ours,a named entity recognition problem,1,High,Med,Med,Med
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,noy.sternlicht@mail.huji.ac.il,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",a natural language inference task,gpt-4o,ontology-based frameworks,1,Med,High,Med,High
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,noy.sternlicht@mail.huji.ac.il,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",a natural language inference task,sciIE,Complex news events,1,Low,High,Med,Low
1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,noy.sternlicht@mail.huji.ac.il,"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",a natural language inference task,mpnet_zero,detecting specific instances of agenda control through social media,1,Med,Low,Med,Low
1-38796_a6183606-5827-4750-892f-2505069e3051,noy.sternlicht@mail.huji.ac.il,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",the human natural language generation process,random,context has been well studied for learning representations,1,Low,Low,Med,Low
1-38796_a6183606-5827-4750-892f-2505069e3051,noy.sternlicht@mail.huji.ac.il,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",the human natural language generation process,ours,recent work that predicts the probabilities of subsequent tokens using multiple heads,1,Low,Low,Low,Low
1-38796_a6183606-5827-4750-892f-2505069e3051,noy.sternlicht@mail.huji.ac.il,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",the human natural language generation process,gpt-4o,recurrent neural networks,1,Low,Low,Low,Low
1-38796_a6183606-5827-4750-892f-2505069e3051,noy.sternlicht@mail.huji.ac.il,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",the human natural language generation process,sciIE,decoder-only large language models (LLMs),1,Med,Low,Med,Low
1-38796_a6183606-5827-4750-892f-2505069e3051,noy.sternlicht@mail.huji.ac.il,"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",the human natural language generation process,mpnet_zero,Decoder-only Large Language Models,1,Med,Low,Med,Low
1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,noy.sternlicht@mail.huji.ac.il,"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.","action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",random,spatial priors embedded in a set of guide functions,1,Low,High,High,Med
1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,noy.sternlicht@mail.huji.ac.il,"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.","action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",ours,a usual syntactic language model,1,Med,Low,Med,Low
1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,noy.sternlicht@mail.huji.ac.il,"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.","action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",gpt-4o,natural language processing techniques,1,High,Low,Low,Low
1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,noy.sternlicht@mail.huji.ac.il,"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.","action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",sciIE,text paraphrasing,1,High,Low,Med,Low
1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,noy.sternlicht@mail.huji.ac.il,"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.","action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",mpnet_zero,an ensemble of a paraphrase LM for prompt (or instruction) rewriting,1,Med,Med,Med,Low
1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,noy.sternlicht@mail.huji.ac.il,"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",the concept of Matryoshka Dolls,random,context-aware decoding,1,Low,Med,Med,Low
1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,noy.sternlicht@mail.huji.ac.il,"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",the concept of Matryoshka Dolls,ours,the token-mixing technique applied in 2D images,1,Low,High,High,Low
1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,noy.sternlicht@mail.huji.ac.il,"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",the concept of Matryoshka Dolls,gpt-4o,biological vision systems,1,Med,High,Med,High
1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,noy.sternlicht@mail.huji.ac.il,"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",the concept of Matryoshka Dolls,sciIE,multimodal representations,1,Low,Low,Med,Low
1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,noy.sternlicht@mail.huji.ac.il,"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",the concept of Matryoshka Dolls,mpnet_zero,Large Multimodal Models,1,High,Low,Med,Low
1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,noy.sternlicht@mail.huji.ac.il,"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",the human decision-making process,random,the feature selection process,1,Low,High,Med,Low
1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,noy.sternlicht@mail.huji.ac.il,"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",the human decision-making process,ours,a sequential decision-making task,1,High,Med,Med,Med
1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,noy.sternlicht@mail.huji.ac.il,"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",the human decision-making process,gpt-4o,reinforcement learning,1,High,Med,Med,Med
1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,noy.sternlicht@mail.huji.ac.il,"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",the human decision-making process,sciIE,controllable text generation approaches,1,Med,Med,Med,Med
1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,noy.sternlicht@mail.huji.ac.il,"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",the human decision-making process,mpnet_zero,Text generation in Large Language Models,1,High,Low,Med,Low
1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,noy.sternlicht@mail.huji.ac.il,"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",a discriminative classification task,random,a self-training mechanism,1,Low,Med,Med,Low
1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,noy.sternlicht@mail.huji.ac.il,"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",a discriminative classification task,ours,a direct-answer-prediction process,1,Low,Low,Low,Low
1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,noy.sternlicht@mail.huji.ac.il,"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",a discriminative classification task,gpt-4o,human conversation patterns,1,Med,Med,Med,Med
1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,noy.sternlicht@mail.huji.ac.il,"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",a discriminative classification task,sciIE,prompt-engineering-based large language models (LLMs),1,Low,Low,Med,Low
1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,noy.sternlicht@mail.huji.ac.il,"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",a discriminative classification task,mpnet_zero,large language models(LLMs),1,High,Low,Med,Low
