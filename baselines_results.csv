example_id,annotator,context,gold,baseline,suggestion,is_ill_defined,knowledge_level,k,rank,normalized_rank
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,gpt-4o,genetic algorithms,False,3,1,2,1.2000000000000002
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,ours,the concept of prompt tuning,False,3,1,5,3.0000000000000004
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,mpnet_zero,prompt-engineering-based large language models,False,3,1,4,2.4000000000000004
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,random,Learning effective representations from raw data,False,3,1,6,3.6000000000000005
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,positive,gradient-based model optimizers,False,3,1,1,0.6000000000000001
1-42304_544e6956-8092-4102-8996-c9d4a7500a75,noy.sternlicht@mail.huji.ac.il,"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",gradient-based model optimizers,sciIE,prompt-engineering-based large language models (LLMs),False,3,1,3,1.8000000000000003
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,sciIE,relation extraction,False,4,1,6,4.800000000000001
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,gpt-4o,transfer learning,False,4,1,3,2.4000000000000004
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,positive,the abundant global knowledge stored within pre-trained language models,False,4,1,1,0.8
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,mpnet_zero,Relation extraction models,False,4,1,5,4.0
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,ours,Question answering over heterogeneous data,False,4,1,2,1.6
1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,noy.sternlicht@mail.huji.ac.il,"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",the abundant global knowledge stored within pre-trained language models,random,a supervised contrastive loss,False,4,1,4,3.2
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,gpt-4o,human cognitive evaluation,False,4,1,3,2.4000000000000004
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,mpnet_zero,the success of large language models in NLP,False,4,1,1,0.8
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,sciIE,fine-tuned large language models (LLMs),False,4,1,2,1.6
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,positive,the principles in subjective question correction,False,4,1,6,4.800000000000001
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,ours,human evaluation,False,4,1,4,3.2
1-23261_dcdf3bfd-5fb5-4416-bf14-0992a328cd59,noy.sternlicht@mail.huji.ac.il,"Modern Large Language Models (LLMs) exhibit a performance gap in basic tasks like relation and event extraction, primarily due to the imprecision of existing evaluation metrics and the incompleteness of evaluation benchmarks caused by restrictive human annotation schemas. These issues lead to an underestimation of LLM performances and highlight the need for improved evaluation methods that can better assess semantic consistency and address benchmark limitations.",the principles in subjective question correction,random,human flexibility and reasoning,False,4,1,5,4.0
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,ours,recent works on machine-generated text detection,False,4,1,3,2.4000000000000004
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,gpt-4o,stylometry techniques,False,4,1,1,0.8
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,random,Adversarial attacks against language models(LMs),False,4,1,4,3.2
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,sciIE,LLM's textual representation counterparts,False,4,1,5,4.0
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,mpnet_zero,an LLM that effectively processes textual information,False,4,1,6,4.800000000000001
1-4330_cca3fb24-9267-4e27-941e-55040f509ca5,noy.sternlicht@mail.huji.ac.il,"There is a growing concern about distinguishing between LLM-generated and human-written texts to prevent misuse, such as the dissemination of misleading information and academic dishonesty. Previous research has primarily focused on classifying text as either entirely human-written or LLM-generated, neglecting the detection of mixed texts that contain both types of content.",a token classification problem,positive,a token classification problem,False,4,1,2,1.6
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,gpt-4o,human linguistic judgment,False,2,1,3,1.2000000000000002
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,ours,human evaluation,False,2,1,1,0.4
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,sciIE,LLM-based automatic evaluation metric,False,2,1,4,1.6
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,random,implicit feature embeddings,False,2,1,5,2.0
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,mpnet_zero,evaluating the quality of text generated by generative Large Language Models(LLMs),False,2,1,6,2.4000000000000004
1-2288_347a15a9-1b5a-49bf-92a8-a2b9d4cee10d,noy.sternlicht@mail.huji.ac.il,"The abstract highlights issues of inconsistent conceptualization and vague expression in existing NLG quality criteria, which reduce the reliability of LLM evaluations. Additionally, it points out the confusion inherent in LLMs regarding different evaluation criteria, indicating a need for further research and improvements in LLM-based evaluation methods.",behavioral testing,positive,behavioral testing,False,2,1,2,0.8
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,mpnet_zero,the language model alignment,False,3,1,2,1.2000000000000002
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,gpt-4o,human collaborative learning,False,3,1,3,1.8000000000000003
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,sciIE,language model fine-tuning techniques,False,3,1,5,3.0000000000000004
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,positive,weak-to-strong generalization,False,3,1,4,2.4000000000000004
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,random,partitioning large-scale hypergraphs,False,3,1,6,3.6000000000000005
1-11323_c67c7988-f949-4d36-bbbc-8c0c132d9a4c,noy.sternlicht@mail.huji.ac.il,"Aligning language models with human preferences is crucial for better meeting diverse user needs, and there is a need to effectively transfer alignment behavior from weaker models to stronger ones. The observation that stronger models can benefit from the alignment capabilities of weaker models highlights a gap in existing approaches to model alignment.",weak-to-strong generalization,ours,aligning Large Language Models with human preferences,False,3,1,1,0.6000000000000001
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,gpt-4o,neuroscience-inspired memory consolidation,False,3,1,4,2.4000000000000004
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,ours,"neuroscience, where the human brain often sheds outdated information to improve the retention of crucial knowledge and facilitate the acquisition of new information",False,3,1,1,0.6000000000000001
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,sciIE,open-source large language models (LLMs),False,3,1,5,3.0000000000000004
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,random,large foundation models,False,3,1,6,3.6000000000000005
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,mpnet_zero,the fusion between distribution of large language models knowledge and distribution of retrieved texts,False,3,1,3,1.8000000000000003
1-31947_b1f88262-1a96-403f-869a-81fc0a4265a7,noy.sternlicht@mail.huji.ac.il,"Large language models (LLMs) often struggle to provide up-to-date information due to their one-time training and the constantly evolving nature of the world. Existing approaches to keep LLMs current face difficulties in extracting stored knowledge, highlighting a need for improved methods of knowledge acquisition from raw documents.",the remarkable success of the Feynman Technique in efficient human learning,positive,the remarkable success of the Feynman Technique in efficient human learning,False,3,1,2,1.2000000000000002
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,ours,the attribution task,False,2,1,4,1.6
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,mpnet_zero,independent disinformation generation characteristics of various large language models,False,2,1,3,1.2000000000000002
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,sciIE,Detecting Misinformation by Integrating Intent featuRes (DM-INTER),False,2,1,1,0.4
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,positive,a domain generalization problem,False,2,1,5,2.0
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,random,a self-evolving framework,False,2,1,6,2.4000000000000004
1-641_e3d9632c-90a7-45f0-a4dc-e89908811948,noy.sternlicht@mail.huji.ac.il,"Model attribution for LLM-generated disinformation is challenging due to the human-like quality of the disinformation produced and the diversity in prompting methods, which complicates accurate source attribution. An effective attribution model must be invariant to domain-specific features and proficient in identifying originating models across various scenarios, reflecting real-world detection challenges.",a domain generalization problem,gpt-4o,forensic linguistics,False,2,1,2,0.8
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,random,a Lagrangian-mechanics-based physical model,False,2,1,6,2.4000000000000004
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,gpt-4o,cultural evolution theory,False,2,1,2,0.8
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,mpnet_zero,studying biases and inherent knowledge of large language modelss,False,2,1,3,1.2000000000000002
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,sciIE,Large Language Model Bias Index (LLMBI),False,2,1,4,1.6
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,positive,causal mediation analysis,False,2,1,5,2.0
1-710_6debd94c-e2b4-4874-a585-24876952adbb,noy.sternlicht@mail.huji.ac.il,"The propagation of social biases within large language models, inherited from diverse training datasets, presents a significant challenge in understanding and mitigating these biases. There is a necessity for tailored debiasing strategies and a deeper understanding of the complex mechanisms and pathways through which bias operates in these models.",causal mediation analysis,ours,the spread of rumors or influence in an online social network,False,2,1,1,0.4
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,gpt-4o,human visual perception,False,2,1,4,1.6
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,positive,evaluating LMs with LMs,False,2,1,2,0.8
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,random,the parameters trained with gradient descent,False,2,1,6,2.4000000000000004
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,ours,human evaluation,False,2,1,3,1.2000000000000002
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,sciIE,multimodal vision-language models (VLMs),False,2,1,5,2.0
1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,noy.sternlicht@mail.huji.ac.il,"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating LMs with LMs,mpnet_zero,evaluate high-level cognitive ability of Large Vision-Language Models using images with rich semantics,False,2,1,1,0.4
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,ours,quantifying the alignment and overlap of concepts across languages in multilingual embeddings,False,3,1,1,0.6000000000000001
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,mpnet_zero,Multilingual neural machine translation models,False,3,1,3,1.8000000000000003
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,positive,the process of integrating semantic features from the source sentences and linguistic features from the target sentences,False,3,1,2,1.2000000000000002
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,sciIE,Pre-trained sequence-to-sequence (seq2seq) multi-lingual models,False,3,1,4,2.4000000000000004
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,random,cognitive load theory,False,3,1,6,3.6000000000000005
1-28499_a7ef1baf-ab5c-453f-915e-f7d3429a8017,noy.sternlicht@mail.huji.ac.il,"To enhance zero-shot translation, models need to share knowledge across languages, which is a challenge in many-to-many multilingual neural machine translation. Existing methods may not effectively leverage both semantic and linguistic features, limiting their ability to improve translation performance across diverse languages.",the process of integrating semantic features from the source sentences and linguistic features from the target sentences,gpt-4o,multilingual pretraining,False,3,1,5,3.0000000000000004
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,ours,a sequential decision-making problem,False,2,1,2,0.8
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,gpt-4o,dynamic neural networks,False,2,1,3,1.2000000000000002
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,sciIE,few-shot capabilities of large language models,False,2,1,1,0.4
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,random,Vision-Language foundation models (VL-models),False,2,1,5,2.0
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,mpnet_zero,recent success of Large Language Models,False,2,1,6,2.4000000000000004
1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,noy.sternlicht@mail.huji.ac.il,"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",a distribution prediction problem,positive,a distribution prediction problem,False,2,1,4,1.6
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,positive,a text generation objective,False,4,1,2,1.6
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,sciIE,information extraction tasks,False,4,1,4,3.2
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,random,parallel distributed compensation,False,4,1,6,4.800000000000001
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,gpt-4o,knowledge graph construction techniques,False,4,1,1,0.8
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,ours,a standard question-answering task,False,4,1,3,2.4000000000000004
1-41666_89a913b3-9f44-4ee4-80d9-4861ab8d754d,noy.sternlicht@mail.huji.ac.il,"The study addresses the challenge of effectively generating AI research leaderboards by extracting specific quadruples from scholarly articles, highlighting the limitations of traditional approaches that rely on predefined taxonomies. It emphasizes the need for improved context selection to enhance the accuracy of large language models and reduce the occurrence of hallucinations in information extraction tasks.",a text generation objective,mpnet_zero,for the Automatic Term Extraction and Classification (ATE) and Classification) tasks,False,4,1,5,4.0
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,ours,"Anderson's ACT-R (Adaptive Control of Thought-Rational), a cognitive architecture modeling human information access and memory dynamics",False,3,1,1,0.6000000000000001
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,random,a biomedical-specialized pre-trained language model,False,3,1,2,1.2000000000000002
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,positive,human learning behavior,False,3,1,3,1.8000000000000003
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,sciIE,Large-Language model (LLM),False,3,1,6,3.6000000000000005
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,gpt-4o,interactive learning systems,False,3,1,5,3.0000000000000004
1-447_47f54a5c-2d5a-405b-817f-76d89fc256ae,noy.sternlicht@mail.huji.ac.il,"Existing RAG models often treat LLMs as passive recipients of information, which can lead to interference from noisy retrieved content. This approach can result in conflicts between external knowledge and parametric memory, highlighting the need for improved engagement and learning from retrieved evidence.",human learning behavior,mpnet_zero,Large Language Models' knowledge recall mechanisms,False,3,1,4,2.4000000000000004
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,positive,multilingual research,False,2,1,2,0.8
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,sciIE,multimodal large language model,False,2,1,5,2.0
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,random,image-based generative AI,False,2,1,3,1.2000000000000002
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,gpt-4o,multimodal transformers,False,2,1,6,2.4000000000000004
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,mpnet_zero,state-of-the-art multimodal large language models,False,2,1,4,1.6
1-31870_974857e0-abd8-4ef1-8ef5-2ef3a634c21f,noy.sternlicht@mail.huji.ac.il,"The internal mechanisms of how multimodal large language models process features from diverse domains remain unexplored, indicating a need for further investigation into the distribution of domain-specific neurons. Additionally, while current models demonstrate Visual Question Answering capability, they do not fully utilize domain-specific information, highlighting a gap in their effectiveness.",multilingual research,ours,the image-to-text mapping process by the multimodal connector,False,2,1,1,0.4
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",sciIE,rule-based reasoners,False,2,1,5,2.0
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",random,unitary weights,False,2,1,6,2.4000000000000004
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",gpt-4o,symbolic logic systems,False,2,1,3,1.2000000000000002
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",mpnet_zero,rule-based logic,False,2,1,4,1.6
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",positive,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",False,2,1,1,0.4
1-40473_92341f82-89ed-4e50-a21c-e848cddb91c1,noy.sternlicht@mail.huji.ac.il,Reasoning about compositional rules is challenging because it requires multiple reasoning steps and attending to the logical relationships between elements. There is a need for effective methods to elicit rule-based reasoning in complex logical expressions.,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",ours,Pearl's structural causal models,False,2,1,2,0.8
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,sciIE,large-scale legal knowledge base,False,4,1,5,4.0
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,mpnet_zero,a multitask benchmark dataset for assessing the Arabic legal knowledge of Large Language Models,False,4,1,4,3.2
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,gpt-4o,case-based reasoning,False,4,1,2,1.6
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,random,Keyphrase Recommendation,False,4,1,6,4.800000000000001
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,positive,human judicial reasoning,False,4,1,3,2.4000000000000004
1-5953_75b9e640-b5b5-4843-b993-87357fa3b599,noy.sternlicht@mail.huji.ac.il,Existing large language models (LLMs) underperform in legal judgment prediction due to challenges in understanding case complexities and distinguishing between similar charges. This highlights a need for improved methodologies that can effectively address these issues to enhance judicial efficiency.,human judicial reasoning,ours,"the Issue, Rule, Application, Conclusion (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers",False,4,1,1,0.8
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,ours,a dependency-sensitive language-modeling problem,False,2,1,4,1.6
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,random,a generic convolution path,False,2,1,5,2.0
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,gpt-4o,cognitive psychology insights,False,2,1,3,1.2000000000000002
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,positive,recent work,False,2,1,6,2.4000000000000004
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,mpnet_zero,Multimodal emotion recognition in conversation and multimodal emotion-cause pair extraction (multimodal emotion-cause pair extraction),False,2,1,2,0.8
1-22875_9e64c8a0-a8f1-438a-b177-a11aa8e6a2a3,noy.sternlicht@mail.huji.ac.il,"Existing methods for Emotion-Cause Pair Extraction tend to overfit spurious correlations, such as positional bias in benchmark datasets, rather than effectively capturing semantic features. Additionally, while large language models have strong capabilities, they suffer from uncontrollable outputs, leading to mediocre performance in this task.",recent work,sciIE,multimodal emotion-cause pair extraction (MECPE),False,2,1,1,0.4
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,random,patient monitoring,False,3,1,6,3.6000000000000005
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,gpt-4o,human cognitive processes,False,3,1,3,1.8000000000000003
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,ours,the fusion between distribution of large language models knowledge and distribution of retrieved texts,False,3,1,2,1.2000000000000002
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,mpnet_zero,the problem faced by Large Language Models,False,3,1,4,2.4000000000000004
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,sciIE,Large-Language model (LLM),False,3,1,5,3.0000000000000004
1-30440_f6e4517a-c74e-4884-92e4-b6df15fcb40b,noy.sternlicht@mail.huji.ac.il,"Large Language Models (LLMs) face challenges due to their overreliance on potentially flawed parametric knowledge, leading to hallucinations and inaccuracies, especially with long-tail, domain-specific queries. Additionally, the presence of noisy and irrelevant information in retrieved long-context documents can dilute LLMs' attention, highlighting the need for improved methods to enhance their contextual awareness and robustness.",the supportive role of essential concepts in individuals' reading comprehension,positive,the supportive role of essential concepts in individuals' reading comprehension,False,3,1,1,0.6000000000000001
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",positive,"the process of identifying, understanding, and regulating emotions",False,3,1,1,0.6000000000000001
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",gpt-4o,cognitive psychology,False,3,1,4,2.4000000000000004
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",mpnet_zero,Emotion Support Conversation,False,3,1,3,1.8000000000000003
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",ours,foundational principles of human communication within psychology,False,3,1,2,1.2000000000000002
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",random,two branch paths from two different multi-scale approaches,False,3,1,5,3.0000000000000004
1-38849_3c50b338-017d-4f6a-8159-dec2b56342d4,noy.sternlicht@mail.huji.ac.il,"Previous works mostly focus on generating better responses but ignore interpretability, which is extremely important for constructing reliable dialogue systems. There is a need to empower systems with better interpretability to enhance the understanding of emotional support responses and establish connections between users and dialogue systems.","the process of identifying, understanding, and regulating emotions",sciIE,language-based interpretability,False,3,1,6,3.6000000000000005
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,ours,evaluating the quality of text generated by generative Large Language Models(LLMs),False,3,1,5,3.0000000000000004
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,sciIE,low consistency between image and text descriptions,False,3,1,3,1.8000000000000003
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,random,the image-text mapping problem,False,3,1,1,0.6000000000000001
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,mpnet_zero,a text-to-image alignment quality prediction task,False,3,1,4,2.4000000000000004
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,positive,the Cobra Effect,False,3,1,2,1.2000000000000002
1-37407_dea6d3d7-f9e5-4d01-8685-22b3b9a2e164,noy.sternlicht@mail.huji.ac.il,"The study identifies deficiencies in reference-free metrics used for evaluating the compatibility between textual descriptions and images, highlighting issues such as incoherent statements and excessive repetition in generated descriptions. This indicates a need for improved evaluation methods that can better align with human judgment and rectify the shortcomings of existing metrics.",the Cobra Effect,gpt-4o,human visual perception,False,3,1,6,3.6000000000000005
