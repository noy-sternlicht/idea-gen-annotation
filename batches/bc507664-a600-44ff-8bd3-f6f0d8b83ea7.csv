context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"Constructing and validating high-quality knowledge repositories for large language models requires considerable effort, and existing methods may not effectively integrate real-time knowledge into these models. Additionally, there is a need for improved performance in both single-document and multi-document question-answering tasks, as current baselines have limitations in efficiency and accuracy.",a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-Retrieval-Augmented Generation),inspiration,"Background: Constructing and validating high-quality knowledge repositories for large language models requires considerable effort, and existing methods may not effectively integrate real-time knowledge into these models. Additionally, there is a need for improved performance in both single-document and multi-document question-answering tasks, as current baselines have limitations in efficiency and accuracy.
Contribution: 'a pre-retrieval framework named Pseudo-Graph Retrieval-Augmented Generation (PG-Retrieval-Augmented Generation)' inspired by ",1,"the human behavior in flipping through notes, identifying fact paths and subsequently exploring the related contexts",1-14430_b5baed40-2fef-4635-b351-4add04259ee7,"['cs.cl', ' cs.ir']",initial heatmaps,Temporal Knowledge Graph Question Answering,graph-based Retrieval-Augmented Generation (RAG) framework,graph neural networks,recent work on knowledge generation from LLMs for text-based QA
"Long-context modeling poses significant challenges for transformer-based large language models due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation from pretraining on short inputs. Existing methods often require sequential access to documents, which may not be necessary for goal-oriented reading, indicating a need for more efficient strategies in processing long documents.",transformers,inspiration,"Background: Long-context modeling poses significant challenges for transformer-based large language models due to the quadratic complexity of the self-attention mechanism and issues with length extrapolation from pretraining on short inputs. Existing methods often require sequential access to documents, which may not be necessary for goal-oriented reading, indicating a need for more efficient strategies in processing long documents.
Contribution: 'transformers' inspired by ",1,human reading behaviors and existing empirical observations,1-34616_d5a50cb3-3804-46c3-b231-cbc888644720,['cs.cl'],consistent-time-sensitive reinforcement learning,Transformer-based large language models,transformer-based language models,Hierarchical processing in the human brain,transformers for sequential data processing
"Despite the remarkable ability of multilingual embeddings to capture linguistic nuances, questions remain about the degree of alignment between languages. This highlights a need for methods that can quantify the alignment and overlap of concepts across different languages within these models.",quantifying the alignment and overlap of concepts across languages in multilingual embeddings,inspiration,"Background: Despite the remarkable ability of multilingual embeddings to capture linguistic nuances, questions remain about the degree of alignment between languages. This highlights a need for methods that can quantify the alignment and overlap of concepts across different languages within these models.
Contribution: 'quantifying the alignment and overlap of concepts across languages in multilingual embeddings' inspired by ",1,research on high-dimensional representations in neural language models,1-11450_84db3c00-d455-4c43-a8b1-40ca26a0a27b,"['cs.cl', ' cs.ai']",a robust tracking control strategy,multilingual embeddings,multi-lingual embedding space,cross-lingual information retrieval,determine if given languages are related
"Large Vision-Language Models (LVLMs) face challenges in adapting to varying computational constraints, which necessitates flexibility in the number of visual tokens to suit different tasks and resources. The exploration of the trade-off between accuracy and computational cost related to the number of visual tokens highlights a significant research need in achieving optimal performance across diverse applications.",Flexible Vision-Language Modeling,inspiration,"Background: Large Vision-Language Models (LVLMs) face challenges in adapting to varying computational constraints, which necessitates flexibility in the number of visual tokens to suit different tasks and resources. The exploration of the trade-off between accuracy and computational cost related to the number of visual tokens highlights a significant research need in achieving optimal performance across diverse applications.
Contribution: 'Flexible Vision-Language Modeling' inspired by ",1,Matryoshka Representation Learning,1-5775_c99d8e49-bf5d-460a-8ae9-640e0b69cb8e,"['cs.cv', ' cs.cl', ' cs.lg']",text representations generated via transformer-based language models,Large Vision-Language Models,Large Vision Language Models (LVLMs),dynamic neural networks,the subword tokenization widely adopted in language models
"Previous studies in multi-modal learning have primarily focused on either inter-modality or intra-modality dependencies in isolation, which may not provide optimal results. This highlights a gap in the existing approaches that do not adequately capture the interactions between different modalities and their relationships to the target label.",Supervised multi-modal learning,inspiration,"Background: Previous studies in multi-modal learning have primarily focused on either inter-modality or intra-modality dependencies in isolation, which may not provide optimal results. This highlights a gap in the existing approaches that do not adequately capture the interactions between different modalities and their relationships to the target label.
Contribution: 'Supervised multi-modal learning' inspired by ",1,generative models,1-292_71d8b1de-8d5c-491c-ac24-cdfc8d80baee,"['cs.cv', ' cs.cl', ' cs.lg']",model-,multi-modal learning,multi-modality learning,attention mechanisms,a Weakly Supervised Cross-modality Contrastive Learning problem
