context,anchor,relation,query,k,positive,id,arxiv_categories,is_cross_domain,random,mpnet_zero,sciIE,gpt-4o,ours
"The confidence calibration of large language models (LLMs) in response to prompts has not been thoroughly explored, despite the significant impact that prompting strategies have on their performance. Existing methods may improve expected calibration but can also lead to over-confidence in certain instances, indicating a need for better approaches to enhance LLM reliability.",LLM,inspiration,"Background: The confidence calibration of large language models (LLMs) in response to prompts has not been thoroughly explored, despite the significant impact that prompting strategies have on their performance. Existing methods may improve expected calibration but can also lead to over-confidence in certain instances, indicating a need for better approaches to enhance LLM reliability.
Contribution: 'LLM' inspired by ",1,human cognition,1-848_5134a364-f0a5-4ab3-bee7-78f5387b88db,['cs.cl'],True,traditional backend optimization methods,existing calibration methods for large language models,zero-shot prompting of large language models (LLMs),human metacognition,uncertainty estimation literature in large language models
"Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.",temporal relation extraction,inspiration,"Background: Existing methods for temporal relation extraction struggle with limited and unevenly distributed annotated data, highlighting a need for more effective approaches to understand task requests in crowdsourcing systems. This gap in research necessitates innovative solutions that can leverage abundant global knowledge to enhance performance in this area.
Contribution: 'temporal relation extraction' inspired by ",1,the abundant global knowledge stored within pre-trained language models,1-37730_6df467e6-c5d7-4a07-9565-99147fcadcdb,['cs.cl'],False,a supervised contrastive loss,Relation extraction models,relation extraction,transfer learning,Question answering over heterogeneous data
"The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.",LLM-based prompt optimizers,inspiration,"Background: The performance of large language models (LLMs) can be significantly enhanced through effective prompt optimization, yet existing methods may not fully leverage the potential of LLMs as prompt optimizers. There is a need for improved strategies that systematically analyze and refine task prompts to achieve better outcomes in various benchmarks.
Contribution: 'LLM-based prompt optimizers' inspired by ",1,gradient-based model optimizers,1-42304_544e6956-8092-4102-8996-c9d4a7500a75,['cs.cl'],False,Learning effective representations from raw data,prompt-engineering-based large language models,prompt-engineering-based large language models (LLMs),genetic algorithms,the concept of prompt tuning
"The ability of large language models (LLMs) to distinguish subtle sentiments remains a challenge, indicating a need for improved methods in sentiment analysis. Existing in-context learning (ICL) approaches may not adequately address sentiment misinterpretation, highlighting a gap in the effectiveness of current techniques.",Large language models,inspiration,"Background: The ability of large language models (LLMs) to distinguish subtle sentiments remains a challenge, indicating a need for improved methods in sentiment analysis. Existing in-context learning (ICL) approaches may not adequately address sentiment misinterpretation, highlighting a gap in the effectiveness of current techniques.
Contribution: 'Large language models' inspired by ",1,the human ability to adjust understanding via feedback,1-23981_af245886-71d0-41f7-80a0-f0df90e5ec51,['cs.cl'],True,spatial features,a sentiment analysis model using local large language models,fine-tuned large language models (LLMs),human emotional intelligence,in-context learning in natural language processing
"Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.",evaluating Vision-Language Models,inspiration,"Background: Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging due to the need to verify both adherence to instructions and the grounding of text outputs in the provided images. This highlights a gap in effective evaluation methods for VLMs, necessitating innovative approaches to improve assessment accuracy and transparency.
Contribution: 'evaluating Vision-Language Models' inspired by ",1,evaluating LMs with LMs,1-34117_868de9d7-8184-4bfc-b655-a3df1c2c4980,['cs.cl'],False,the parameters trained with gradient descent,evaluate high-level cognitive ability of Large Vision-Language Models using images with rich semantics,multimodal vision-language models (VLMs),human visual perception,human evaluation
