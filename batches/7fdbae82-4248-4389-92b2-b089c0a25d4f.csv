context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",the design of AI writing tools,inspiration,"Background: This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.
Contribution: 'the design of AI writing tools' inspired by ",1,avant-garde literature,1-5936_93733f00-5c88-4125-947d-5061e595fcfd,"['cs.hc', ' cs.cl']",LLM-based logits,Flower and Hayes' cognitive process theory of writing,writing systems,literary history,Flower and Hayes' cognitive process theory of writing
"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",model generation,inspiration,"Background: Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.
Contribution: 'model generation' inspired by ",1,a thread of execution,1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,['cs.cl'],little human input,improve the performance of Large Language Models on complex reasoning and planning tasks,Large Language Model,cognitive psychology problem-solving strategies,complex task-solving processes
"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",the early exiting problem,inspiration,"Background: Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.
Contribution: 'the early exiting problem' inspired by ",1,a distribution prediction problem,1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,['cs.cl'],Vision-Language foundation models (VL-models),recent success of Large Language Models,few-shot capabilities of large language models,dynamic neural networks,a sequential decision-making problem
"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.",user intent,inspiration,"Background: Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.
Contribution: 'user intent' inspired by ",1,"action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,"['cs.cl', ' cs.ai', ' cs.lg']",spatial priors embedded in a set of guide functions,an ensemble of a paraphrase LM for prompt (or instruction) rewriting,text paraphrasing,natural language processing techniques,a usual syntactic language model
"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",propositional logic,inspiration,"Background: The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.
Contribution: 'propositional logic' inspired by ",1,translating text into an Abstract Meaning Representation graph,1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,"['cs.ai', ' cs.cl']",machine learning behavior models,Natural Language Inference models,natural language inference,truth tables,Miller's cognitive model of explanation
