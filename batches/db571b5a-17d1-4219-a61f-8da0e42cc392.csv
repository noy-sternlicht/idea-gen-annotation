context,anchor,relation,query,k,positive,id,random,mpnet_zero,sciIE,gpt-4o,ours
"The effectiveness of few-shot examples in in-context learning can be improved by redefining the notion of relevance specific to the utility for downstream tasks, as standard unsupervised ranking models may not optimally select these examples. Additionally, there is a need for a task-specific approach to enhance the accuracy of predictions made using large language models.",in-context learning,inspiration,"Background: The effectiveness of few-shot examples in in-context learning can be improved by redefining the notion of relevance specific to the utility for downstream tasks, as standard unsupervised ranking models may not optimally select these examples. Additionally, there is a need for a task-specific approach to enhance the accuracy of predictions made using large language models.
Contribution: 'in-context learning' inspired by ",2,"a non-parametric approach, such as $k$-NN",2-34982_cf159bd2-a416-41c4-8e84-a77b522b819d,the versatility of sinusoidal representation networks,the in-context learning feature of Large Language Models,contextual relevance,cognitive psychology,recent work in few-shot learning
"Current multi-modal large language models face challenges in detailed multi-modal understanding, complex task comprehension, and reasoning over multi-modal information. There is a need for improved performance in complex visual reasoning tasks, as existing models have inherent limitations in these areas.",Multi-modal Critical Thinking Agent,inspiration,"Background: Current multi-modal large language models face challenges in detailed multi-modal understanding, complex task comprehension, and reasoning over multi-modal information. There is a need for improved performance in complex visual reasoning tasks, as existing models have inherent limitations in these areas.
Contribution: 'Multi-modal Critical Thinking Agent' inspired by ",2,human cognitive processes and critical thinking,2-30926_f4c8741d-3ec3-4fca-90a5-266ce3ec1bea,giant foundation models,recent success of leveraging large language models for visual reasoning,multimodal comprehension,cognitive neuroscience of human perception,"the human systems of ""thinking fast and slow"" as introduced by Kahneman"
"Raising users' privacy awareness is crucial in modern technology environments, yet existing efforts have largely failed to systematically address the challenge of motivating users to engage with privacy-related information. This gap highlights the need for effective design strategies that can enhance user motivation and attention to privacy issues.",design ideas and categories dedicated to motivating users to engage with privacy-related information,inspiration,"Background: Raising users' privacy awareness is crucial in modern technology environments, yet existing efforts have largely failed to systematically address the challenge of motivating users to engage with privacy-related information. This gap highlights the need for effective design strategies that can enhance user motivation and attention to privacy issues.
Contribution: 'design ideas and categories dedicated to motivating users to engage with privacy-related information' inspired by ",1,the Protection Motivation Theory,1-17882_6478514a-bb2e-4da2-a2c7-60fa9557eaee,coarse-to-fine human cognitive mechanisms,privacy-enhanced mobile data,reduced privacy concerns,behavioral economics,theories of behavior change that emphasize not only technical solutions but also the behavioral responsibility of practitioners
"Conventional approaches to Dynamic Scene Graph Generation often rely on multi-stage pipelines, which can lead to sub-optimal solutions due to the separation and independent optimization of various stages. Additionally, capturing temporal dependencies in videos presents a significant challenge, as existing methods may require additional trackers or handcrafted trajectories, limiting their effectiveness.",Dynamic Scene Graph Generation,inspiration,"Background: Conventional approaches to Dynamic Scene Graph Generation often rely on multi-stage pipelines, which can lead to sub-optimal solutions due to the separation and independent optimization of various stages. Additionally, capturing temporal dependencies in videos presents a significant challenge, as existing methods may require additional trackers or handcrafted trajectories, limiting their effectiveness.
Contribution: 'Dynamic Scene Graph Generation' inspired by ",1,a set prediction problem,1-7746_8761af15-b7c9-48f3-9201-5ce1d52e35ce,current sessions,Scene graph generation,Lifelong Scene Graph Generation (LSGG),transformer models,a spatiotemporal graph sequence nowcasting problem
"Raising users' privacy awareness is crucial in modern technology environments, yet existing efforts have largely failed to systematically address the challenge of motivating users to engage with privacy-related information. This gap highlights the need for effective design strategies that can enhance user motivation and attention to privacy issues.",design ideas and categories dedicated to motivating users to engage with privacy-related information,inspiration,"Background: Raising users' privacy awareness is crucial in modern technology environments, yet existing efforts have largely failed to systematically address the challenge of motivating users to engage with privacy-related information. This gap highlights the need for effective design strategies that can enhance user motivation and attention to privacy issues.
Contribution: 'design ideas and categories dedicated to motivating users to engage with privacy-related information' inspired by ",2,the Protection Motivation Theory,2-17882_6478514a-bb2e-4da2-a2c7-60fa9557eaee,a pseudo-3D scene,the Protection Motivation Theory,privacy,behavioral economics,the Protection Motivation Theory
"Large Vision-Language Models (LVLMs) face challenges in adapting to varying computational constraints, which necessitates flexibility in the number of visual tokens to suit different tasks and resources. The exploration of the trade-off between accuracy and computational cost related to the number of visual tokens highlights a significant research need in achieving optimal performance across diverse applications.",Flexible Vision-Language Modeling,inspiration,"Background: Large Vision-Language Models (LVLMs) face challenges in adapting to varying computational constraints, which necessitates flexibility in the number of visual tokens to suit different tasks and resources. The exploration of the trade-off between accuracy and computational cost related to the number of visual tokens highlights a significant research need in achieving optimal performance across diverse applications.
Contribution: 'Flexible Vision-Language Modeling' inspired by ",1,Matryoshka Representation Learning,1-5775_c99d8e49-bf5d-460a-8ae9-640e0b69cb8e,text representations generated via transformer-based language models,Large Vision-Language Models,Large Vision Language Models (LVLMs),dynamic neural networks,the subword tokenization widely adopted in language models
"The costly training of implicit neural representations (INRs) presents a significant challenge, necessitating efficient strategies for learning nonparametrically defined target functions, such as those represented by image functions. Additionally, there is a need for improved training efficiency in INR learning to achieve faster convergence and reduce training time across various input modalities.",learning of implicit neural representation,inspiration,"Background: The costly training of implicit neural representations (INRs) presents a significant challenge, necessitating efficient strategies for learning nonparametrically defined target functions, such as those represented by image functions. Additionally, there is a need for improved training efficiency in INR learning to achieve faster convergence and reduce training time across various input modalities.
Contribution: 'learning of implicit neural representation' inspired by ",1,a nonparametric teaching problem,1-42199_8085d0f9-5116-4519-96c4-e56fd0ed581c,noisy inverse problems,Neural implicit representations,Implicit Neural Representation (INR),meta-learning techniques,the learning of neural signed distance functions
"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",Large Language Models,inspiration,"Background: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.
Contribution: 'Large Language Models' inspired by ",2,the human natural language generation process,2-38796_a6183606-5827-4750-892f-2505069e3051,an S-D Encoder that explicitly models semantic and depth priors,"Small language models, like Phi2, Gemma, and DistilBERT,are streamlined versions of large language models with fewer parameters that require less power and memory to run",large decoder language model,recurrent neural networks,speculative decoding
"The current approach to processing event data often converts it into frame-based representations, which neglects the sparsity of event data and loses fine-grained temporal information, leading to an increased computational burden and making it ineffective for characterizing event camera properties. Additionally, there exists a performance gap between point-based methods and frame-based methods, indicating a need for improved techniques that can effectively leverage the unique characteristics of event cameras.",Event cameras,inspiration,"Background: The current approach to processing event data often converts it into frame-based representations, which neglects the sparsity of event data and loses fine-grained temporal information, leading to an increased computational burden and making it ineffective for characterizing event camera properties. Additionally, there exists a performance gap between point-based methods and frame-based methods, indicating a need for improved techniques that can effectively leverage the unique characteristics of event cameras.
Contribution: 'Event cameras' inspired by ",2,biological systems,2-1337_60124878-79fa-4911-a7f5-2cde6f215782,fast,processing data from event cameras,event camera,spiking neural networks,event cameras whose output data can directly feed these neural network inputs
"When there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations can limit efficient learning for the Student agents. This highlights the need for a framework that can effectively address the challenge of heterogeneity between Teacher and Student agents to improve learning outcomes in control tasks within sparse-reward environments.",enable the Teacher to detect and adapt to differences between itself and the Student,inspiration,"Background: When there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations can limit efficient learning for the Student agents. This highlights the need for a framework that can effectively address the challenge of heterogeneity between Teacher and Student agents to improve learning outcomes in control tasks within sparse-reward environments.
Contribution: 'enable the Teacher to detect and adapt to differences between itself and the Student' inspired by ",2,"the concept of ""surprise""",2-11544_39f45f1e-9368-469c-ad24-c473ec517ec0,Artificial Intelligence Auditability,policy learning from expert demonstrations,parameterized locomotion skills,transfer learning,principles from operant learning psychology
