context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"Adapting large language models (LLMs) to novel tasks remains challenging, particularly as colossal models have high computational demands that limit their widespread use, while smaller models struggle without context. This study addresses the need for improved generalization from labeled examples of predefined tasks to novel tasks, highlighting a gap in the ability of LLMs to effectively utilize contextual signals from different task examples.",Large Language Models,inspiration,"Background: Adapting large language models (LLMs) to novel tasks remains challenging, particularly as colossal models have high computational demands that limit their widespread use, while smaller models struggle without context. This study addresses the need for improved generalization from labeled examples of predefined tasks to novel tasks, highlighting a gap in the ability of LLMs to effectively utilize contextual signals from different task examples.
Contribution: 'Large Language Models' inspired by ",1,biological neurons and the mechanistic interpretation of the Transformer architecture,1-28664_3ba423e6-05ac-4bf2-8939-0ef4cf01710f,['cs.cl'],Image-goal navigation,the cross-task generality of Large Language Models on NLP tasks,fine-tuned large language models (LLMs),meta-learning techniques,the zero-shot ability of large language models to generalize to novel contexts
"The study addresses the potential biases in multilingual sentiment analysis, particularly between English and French, highlighting the need for equitable treatment across languages in Natural Language Processing systems. It also emphasizes the importance of incorporating diverse datasets in future research to better understand and mitigate these biases.",potential biases in multilingual sentiment analysis between English and French,inspiration,"Background: The study addresses the potential biases in multilingual sentiment analysis, particularly between English and French, highlighting the need for equitable treatment across languages in Natural Language Processing systems. It also emphasizes the importance of incorporating diverse datasets in future research to better understand and mitigate these biases.
Contribution: 'potential biases in multilingual sentiment analysis between English and French' inspired by ",1,the 'Bias Considerations in Bilingual Natural Language Processing' report by Statistics Canada,1-13610_bc0ae327-8878-4997-8556-95e64069cf0b,['cs.cl'],ViTs,the 'Bias Considerations in Bilingual Natural Language Processing' report by Statistics Canada,sentiment polarities,cross-linguistic sentiment datasets,quantifying the alignment and overlap of concepts across languages in multilingual embeddings
"The lack of widely acknowledged testing mechanisms for evaluating large language models (LLMs) has led to uncertainty regarding their comprehension capabilities, particularly in understanding nuanced semantics. Existing research primarily focuses on surface-level natural language understanding, neglecting the fine-grained explorations necessary for a deeper understanding of LLMs' unique comprehension mechanisms and their alignment with human cognition.",Large language models,inspiration,"Background: The lack of widely acknowledged testing mechanisms for evaluating large language models (LLMs) has led to uncertainty regarding their comprehension capabilities, particularly in understanding nuanced semantics. Existing research primarily focuses on surface-level natural language understanding, neglecting the fine-grained explorations necessary for a deeper understanding of LLMs' unique comprehension mechanisms and their alignment with human cognition.
Contribution: 'Large language models' inspired by ",1,foundational principles of human communication within psychology,1-18678_228afba2-62a8-47e6-95e9-955b0f596c9d,"['cs.cl', ' cs.ai']",a dynamic observation module,"positioning Large Language Models as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning",Large-Language model (LLM),cognitive neuroscience,human cognitive science
"The limited controllability of large language models (LLMs) poses a significant challenge for downstream applications, particularly in the context of language generation. Existing LLMs rely on fully auto-regressive generation, which does not adequately address the intricacies of decision-making involved in language processing.",a decision-making process,inspiration,"Background: The limited controllability of large language models (LLMs) poses a significant challenge for downstream applications, particularly in the context of language generation. Existing LLMs rely on fully auto-regressive generation, which does not adequately address the intricacies of decision-making involved in language processing.
Contribution: 'a decision-making process' inspired by ",1,"the neural mechanisms of the human brain, specifically Broca's and Wernicke's areas, which are crucial for language generation and comprehension",1-32037_336b2c18-9823-4bfc-92e2-c13a60676a01,"['cs.cl', ' cs.lg']",auxiliary objective,Text generation in Large Language Models,Large Language Model,human cognitive decision-making,a one-step Markov decision process
"Large Language Models (LLMs) often struggle with domain-specific needs due to their generic knowledge, and fine-tuning can lead to catastrophic forgetting and reduced generalizability. There is a need for effective demonstration retrieval systems that prioritize discovery and maximize information gain, as traditional relevance measures may not suffice in this context.",Fine-tuning of Large Language Models,inspiration,"Background: Large Language Models (LLMs) often struggle with domain-specific needs due to their generic knowledge, and fine-tuning can lead to catastrophic forgetting and reduced generalizability. There is a need for effective demonstration retrieval systems that prioritize discovery and maximize information gain, as traditional relevance measures may not suffice in this context.
Contribution: 'Fine-tuning of Large Language Models' inspired by ",1,item-cold-start recommender systems,1-15871_6b6fcf1b-6ead-461e-a437-55347cefc0b3,"['cs.ir', ' cs.cl', ' cs.lg']",human-like heuristics and shortcuts,retrieval-augmented Large language models,fine-tuned large language models (LLMs),meta-learning techniques,a strategy named Collaborative Knowledge-STudying-Enhanced Evolution by Retrieval (Co-STEER)
