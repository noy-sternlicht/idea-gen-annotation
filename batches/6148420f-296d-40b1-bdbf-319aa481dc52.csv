context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
The examination of online influence campaigns often faces challenges due to the limited or non-existent annotated data available for analysis. This limitation necessitates the development of methodologies that can effectively detect instances of agenda control without relying on large annotated training datasets.,detecting specific instances of agenda control through social media,inspiration,"Background: The examination of online influence campaigns often faces challenges due to the limited or non-existent annotated data available for analysis. This limitation necessitates the development of methodologies that can effectively detect instances of agenda control without relying on large annotated training datasets.
Contribution: 'detecting specific instances of agenda control through social media' inspired by ",1,a textual entailment problem,1-1231_515703b6-01ae-4c64-8b62-818a78428427,['cs.cl'],a multi-stage grasping network,Misinformation detection on social media,artificial social influence,unsupervised learning techniques,a social science problem
"Fine-tuning large language models can be challenging, and existing methods may not effectively improve alignment without additional training. There is a need for compute-efficient strategies that enhance model performance while addressing the limitations of traditional tuning approaches.",framing the alignment of a large language model as a test-time greedy search,inspiration,"Background: Fine-tuning large language models can be challenging, and existing methods may not effectively improve alignment without additional training. There is a need for compute-efficient strategies that enhance model performance while addressing the limitations of traditional tuning approaches.
Contribution: 'framing the alignment of a large language model as a test-time greedy search' inspired by ",1,a test-time greedy search to maximize the log-likelihood difference between small tuned and untuned models while sampling from the frozen large model,1-34631_9bd9fc9f-a6b9-4f59-b02f-75c3b828f350,"['cs.cl', ' cs.ai', ' cs.lg']",Field boundary delineation,alignment in tuning large language models,fine-tuned large language models (LLMs),Monte Carlo Tree Search,a heuristic search problem
"Current multi-modal large language models face challenges in detailed multi-modal understanding, complex task comprehension, and reasoning over multi-modal information. There is a need for improved performance in complex visual reasoning tasks, as existing models have inherent limitations in these areas.",Multi-modal Critical Thinking Agent,inspiration,"Background: Current multi-modal large language models face challenges in detailed multi-modal understanding, complex task comprehension, and reasoning over multi-modal information. There is a need for improved performance in complex visual reasoning tasks, as existing models have inherent limitations in these areas.
Contribution: 'Multi-modal Critical Thinking Agent' inspired by ",1,human cognitive processes and critical thinking,1-30926_f4c8741d-3ec3-4fca-90a5-266ce3ec1bea,"['cs.cl', ' cs.ai', ' cs.cv', ' cs.lg']",Debiasing,the visual-centric reasoning module empowered by Multi-modal Large Language Model,multi-modal critical thinking agent framework,cognitive neuroscience of human perception,human cognitive processes and critical thinking
"Large language models (LLMs) struggle to efficiently and effectively integrate a large amount of new experiences after pre-training, often leading to issues such as catastrophic forgetting. Existing retrieval-augmented generation methods do not adequately address the need for deeper and more efficient knowledge integration over new experiences.",Large language models,inspiration,"Background: Large language models (LLMs) struggle to efficiently and effectively integrate a large amount of new experiences after pre-training, often leading to issues such as catastrophic forgetting. Existing retrieval-augmented generation methods do not adequately address the need for deeper and more efficient knowledge integration over new experiences.
Contribution: 'Large language models' inspired by ",1,the hippocampal indexing theory of human long-term memory,1-22754_bedeb6c2-8c39-419a-a266-853073a26761,"['cs.cl', ' cs.ai']",clinicians' decision-making processes for hemoglobin level/anemia degree prediction,"Contrary to human language learning, recent advancements in large language models have primarily adopted a non-interactive training paradigm, and refined pre-trained models through feedback afterward",long-context retrieval augmented generation,the memory consolidation process of the human brain,the retrieve-then-generate paradigm of Retrieval-augmented generation
"The study addresses the challenge of promoting constructive discussions on controversial topics online, particularly by transforming disagreeing responses to signal receptiveness. It highlights the need for a framework that aligns generated responses with human perceptions of receptiveness, which is crucial for effective content moderation.",automatic reframing of disagreeing responses,inspiration,"Background: The study addresses the challenge of promoting constructive discussions on controversial topics online, particularly by transforming disagreeing responses to signal receptiveness. It highlights the need for a framework that aligns generated responses with human perceptions of receptiveness, which is crucial for effective content moderation.
Contribution: 'automatic reframing of disagreeing responses' inspired by ",1,"research from psychology, communications, and linguistics",1-21937_ac207314-75f0-42a8-af87-f22b8f044c8c,['cs.cl'],cross-attention maps,human behavior in debate preparation and execution,sentiment polarities,politeness strategies,"research from psychology, communications, and linguistics"
