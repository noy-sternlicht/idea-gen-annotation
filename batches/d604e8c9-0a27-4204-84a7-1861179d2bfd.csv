context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",text generation,inspiration,"Background: Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.
Contribution: 'text generation' inspired by ",1,the human decision-making process,1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,"['cs.cl', ' cs.ai']",the feature selection process,Text generation in Large Language Models,controllable text generation approaches,reinforcement learning,a sequential decision-making task
"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Post-training Quantization of Large Language Models (Large Language Models),inspiration,"Background: The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.
Contribution: 'Post-training Quantization of Large Language Models (Large Language Models)' inspired by ",1,Adaptive LASSO regression model,1-29581_8487266a-eb35-4090-9193-90e818d727f7,['cs.cl'],an energy score,existing calibration methods for large language models,few-shot capabilities of large language models,zero-shot learning,the influence of label smoothing on model confidence and differential privacy
"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",patent prosecution,inspiration,"Background: The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.
Contribution: 'patent prosecution' inspired by ",1,a system of reinforcement learning from human feedback,1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,['cs.cl'],image-based conditioning through cross-attention,the evaluation of generated text,human expert proofreading,active learning techniques,a system of reinforcement learning from human feedback
"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",giving deep networks a prior grounded in explicit medical knowledge communicated in natural language,inspiration,"Background: Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.
Contribution: 'giving deep networks a prior grounded in explicit medical knowledge communicated in natural language' inspired by ",1,medical training,1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,"['cs.cv', ' cs.cl']",encoder-decoder model,enhance the capability of medical Multimodal Large Languange Models in understanding anatomical regions within entire medical scans,semantic high dimensional medical knowledge,clinical guidelines,"Inspired by this, we investigate the role of medical knowledge in disease diagnosis through doctor-patient interaction"
"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",enhance LLM creativity,inspiration,"Background: Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.
Contribution: 'enhance LLM creativity' inspired by ",1,emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,"['cs.cl', ' cs.ai']",the human review process,assessing the level of creativity in large language models,open language models,human creative processes,emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives
