context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"Existing methods for pose estimation often require expensive human annotation of contact points and specialized models, which can be limiting. Additionally, there is a need for a unified framework that effectively resolves both self-contact and person-to-person contact in pose estimation tasks.",pose estimation,inspiration,"Background: Existing methods for pose estimation often require expensive human annotation of contact points and specialized models, which can be limiting. Additionally, there is a need for a unified framework that effectively resolves both self-contact and person-to-person contact in pose estimation tasks.
Contribution: 'pose estimation' inspired by ",1,language is often used to describe physical interaction,1-29468_e80e98c7-02f4-4eae-acff-f4492c597e47,"['cs.cv', ' cs.cl']",a token classification problem,3D human pose estimation,human pose estimation,graph neural networks,"a contact-state transition based on three action representations (detach, crossover, attach)"
"The process extraction task in scientific publications faces challenges related to performance accuracy, particularly in low-resource settings such as the chemistry domain. Additionally, there is a need to reduce overfitting when training on small datasets, which can hinder the effectiveness of existing models.",process extraction,inspiration,"Background: The process extraction task in scientific publications faces challenges related to performance accuracy, particularly in low-resource settings such as the chemistry domain. Additionally, there is a need to reduce overfitting when training on small datasets, which can hinder the effectiveness of existing models.
Contribution: 'process extraction' inspired by ",1,a sequence labeling task,1-885_a1784a6a-4a03-446f-af18-387ad2f70cb8,"['cs.cl', ' cs.ir']","frame the Bayesian optimization problem in natural language, enabling Large Language Modelss to iteratively propose and evaluate promising solutions conditioned on historical evaluations",the strong capabilities of Large Language Models in procedure understanding and text summarization,document-level event extraction,transfer learning techniques,joint entity and relation extraction from unstructured text
"Prior work on retrieval-augmented generation models has relied on simplifying assumptions of marginalization and document independence, which may limit their effectiveness. There is a need for improved optimization methods that can address these limitations and enhance performance across diverse tasks.",the retrieval process in retrieval-augmented generation,inspiration,"Background: Prior work on retrieval-augmented generation models has relied on simplifying assumptions of marginalization and document independence, which may limit their effectiveness. There is a need for improved optimization methods that can address these limitations and enhance performance across diverse tasks.
Contribution: 'the retrieval process in retrieval-augmented generation' inspired by ",1,a stochastic sampling without replacement process,1-26059_087e9d03-6d5b-403f-93d4-5958c9ebb3fa,"['cs.cl', ' cs.ir', ' cs.lg']",a Scene Parser Network to transform static-dynamic video scenes into Symbolic Representation (Symbolic Representation),a modified Retrieval-Augmented Generation pipeline,retrieval augmented generation,information retrieval systems,an embedding-based retrieval process
"Transformer-based large language models are limited by their context window, which restricts their ability to attend to all input tokens. Previous models have struggled with memory architectures that are not effective in selecting and filtering information, highlighting a need for improved long-context processing capabilities.",Transformer-based large language models,inspiration,"Background: Transformer-based large language models are limited by their context window, which restricts their ability to attend to all input tokens. Previous models have struggled with memory architectures that are not effective in selecting and filtering information, highlighting a need for improved long-context processing capabilities.
Contribution: 'Transformer-based large language models' inspired by ",1,humans are good at learning and self-adjustment,1-37488_d9edf025-436f-4199-89aa-885bc3dc757d,"['cs.cl', ' cs.lg']",goal-conditioned generative models,a Transformer-XL large language model,transformer-based language models,hierarchical attention mechanisms,Transformers employ a similar notion of attention in their architecture
"Current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies, indicating a need for more effective methods to enhance mistake detection. The challenges in accurately identifying mathematical reasoning mistakes highlight a gap in existing approaches that the proposed methodology aims to address.","a unique prompting strategy, termed the Pedagogical Chain-of-Thought, which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes",inspiration,"Background: Current literature suggests that LLMs often struggle with reliably identifying reasoning mistakes when using simplistic prompting strategies, indicating a need for more effective methods to enhance mistake detection. The challenges in accurately identifying mathematical reasoning mistakes highlight a gap in existing approaches that the proposed methodology aims to address.
Contribution: 'a unique prompting strategy, termed the Pedagogical Chain-of-Thought, which is specifically designed to guide the identification of reasoning mistakes, particularly mathematical reasoning mistakes' inspired by ",1,the educational theory of the Bloom Cognitive Model,1-16722_89d45914-f370-4ecb-8468-641dac84d6fb,"['cs.cl', ' cs.ai']",estimated robot poses,the Chain-of-Thought which prompts LLMs to address problems step-by-step,identification of reasoning mistakes,Socratic questioning techniques,how humans approach math pedagogically
