context,anchor,relation,query,k,positive,id,arxiv_categories,random,mpnet_zero,sciIE,gpt-4o,ours
"The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.",an explanatory feedback system,inspiration,"Background: The high demand for qualified tutors presents a challenge, often necessitating the training of novice tutors to ensure effective tutoring. Additionally, providing timely explanatory feedback to facilitate the training process is complicated by the time-consuming nature of assessing trainee performance by human experts.
Contribution: 'an explanatory feedback system' inspired by ",1,recent advancements of large language models,1-327_6d14eaa3-b3dc-4204-8164-b91b21bf5915,"['cs.cl', ' cs.ai', ' cs.hc']",the cumulative sum statistic,build dialog tutoring models that scaffold students' problem-solving,AI tutors,intelligent tutoring systems,students' Intelligent Tutoring System performance metrics
"The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.",training of large language models,inspiration,"Background: The rapid advancement of large language models has improved text understanding and generation but poses challenges in computational resources and scalability during training. Traditional random data shuffling may not effectively enhance performance, indicating a need for more structured training approaches to address these limitations.
Contribution: 'training of large language models' inspired by ",1,a curriculum learning process,1-39276_43781121-205c-457f-8985-5aa9fc712ef0,"['cs.cl', ' cs.ai']",a dynamic observation module,the training processes of large language models,Large Language Model,curriculum learning,knowledge transferring in multi-task learning
"The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.",stakeholder classification,inspiration,"Background: The automated detection of stakeholder roles within news content remains an underexplored domain, despite existing works focusing on salient entity extraction and political affiliations through social media data. Recognizing the various types of news stakeholders and their roles is crucial for a nuanced comprehension of news narratives.
Contribution: 'stakeholder classification' inspired by ",1,a natural language inference task,1-5354_99c25e0f-0e43-4381-b22e-afb3e9f3ded5,"['cs.cl', ' cs.ir']",a more robust and efficient control mechanism for autonomous manipulation in robot arms,detecting specific instances of agenda control through social media,Complex news events,ontology-based frameworks,a named entity recognition problem
"The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.",medical image recognition,inspiration,"Background: The complexity of medical image recognition is heightened by the presence of varied pathological indications, which poses significant challenges in multi-label classification, particularly with unseen labels. Existing methods often rely on manual prompt construction by expert radiologists and struggle with generalizability in fine-grained scenarios, highlighting the need for more efficient and adaptable approaches.
Contribution: 'medical image recognition' inspired by ",1,text generation in natural language processing,1-16286_486004d5-f250-476c-8346-a56ced165a62,"['cs.cv', ' cs.cl']",3D hand poses at a high temporal resolution for fine-grained motion modeling,multi-label image classification,medical image classification,self-supervised learning,medical visual question answering (Med-VQA)
"Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.",AI agents,inspiration,"Background: Modern AI systems often achieve superhuman performance but lack essential human-like features such as generalization, interpretability, and human inter-operability. There is a need for AI agents to learn more interpretable and generalizable behaviors that can facilitate effective human-AI coordination.
Contribution: 'AI agents' inspired by ",1,the rich interactions between language and decision-making in humans,1-7599_d93e817d-dabb-4601-8609-10eebdd92a95,"['cs.lg', ' cs.ai', ' cs.cl']",visual tokens,replicating human-like cognitive processes in AI,AI agents,cognitive neuroscience,Humans can quickly learn new behaviors by leveraging background world knowledge
"This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.",the design of AI writing tools,inspiration,"Background: This essay explores how literary concepts can advance design theory around AI writing tools, encouraging creators to engage with past writing innovations alongside new technological possibilities. It also addresses the need for a critical perspective on the concerns writers have historically expressed, which may inform the design of contemporary writing tools.
Contribution: 'the design of AI writing tools' inspired by ",1,avant-garde literature,1-5936_93733f00-5c88-4125-947d-5061e595fcfd,"['cs.hc', ' cs.cl']",LLM-based logits,Flower and Hayes' cognitive process theory of writing,writing systems,literary history,Flower and Hayes' cognitive process theory of writing
"Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.",model generation,inspiration,"Background: Large language models (LLMs) struggle with increased context length and complexity, leading to challenges in task solving and question answering. Existing frameworks do not adequately address the need for models to dynamically adapt and decompose complex tasks into simpler sub-problems for improved performance.
Contribution: 'model generation' inspired by ",1,a thread of execution,1-40634_3bb890a9-899a-427b-9c31-2ccf4c136df0,['cs.cl'],little human input,improve the performance of Large Language Models on complex reasoning and planning tasks,Large Language Model,cognitive psychology problem-solving strategies,complex task-solving processes
"Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.",the early exiting problem,inspiration,"Background: Existing methods for early exiting in large language models require significant effort to train internal classifiers and can only achieve comparable performance at best, highlighting a need for more efficient approaches. Additionally, the high computational overhead associated with model inference presents a challenge that necessitates innovative solutions to accelerate inference while maintaining performance.
Contribution: 'the early exiting problem' inspired by ",1,a distribution prediction problem,1-7666_2a8784a6-8ba4-427a-83c8-f512ec6d752c,['cs.cl'],Vision-Language foundation models (VL-models),recent success of Large Language Models,few-shot capabilities of large language models,dynamic neural networks,a sequential decision-making problem
"Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.",user intent,inspiration,"Background: Existing methods for controlled paraphrase generation often require detailed parse trees or syntactic exemplars, which do not align with human-like paraphrasing behavior. Additionally, there is an inference gap where control specifications are only available during training, limiting the model's ability to operate effectively during inference.
Contribution: 'user intent' inspired by ",1,"action tokens, embedding and concatenating them with text embeddings, thus flowing together into a self-attention encoder for representation fusion",1-31210_17837f1a-230e-45c4-bd83-f4394a1c55b6,"['cs.cl', ' cs.ai', ' cs.lg']",spatial priors embedded in a set of guide functions,an ensemble of a paraphrase LM for prompt (or instruction) rewriting,text paraphrasing,natural language processing techniques,a usual syntactic language model
"The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.",propositional logic,inspiration,"Background: The need for an explainable approach to natural language inference is highlighted, as existing machine learning and deep learning solutions often lack transparency and explicitness. Additionally, the challenge of identifying entailment or contradiction relationships due to varying wordings in logical representations suggests a gap in current methodologies that this research aims to address.
Contribution: 'propositional logic' inspired by ",1,translating text into an Abstract Meaning Representation graph,1-39526_cf84a7a5-66bb-4598-b7c4-a78840132ec2,"['cs.ai', ' cs.cl']",machine learning behavior models,Natural Language Inference models,natural language inference,truth tables,Miller's cognitive model of explanation
"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",Learned Visual Content Representation,inspiration,"Background: The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.
Contribution: 'Learned Visual Content Representation' inspired by ",1,the concept of Matryoshka Dolls,1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,"['cs.cv', ' cs.ai', ' cs.cl', ' cs.lg']",context-aware decoding,Large Multimodal Models,multimodal representations,biological vision systems,the token-mixing technique applied in 2D images
"The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.",the language modeling task,inspiration,"Background: The use of large language models (LLMs) in customer support applications is challenged by their tendency to hallucinate, which complicates their practical implementation. This necessitates a new approach to effectively leverage LLMs while mitigating their limitations in generating reliable responses.
Contribution: 'the language modeling task' inspired by ",1,a discriminative classification task,1-17133_f28a22ed-aa09-406d-8f6e-e9c33da21483,"['cs.cl', ' cs.lg']",a self-training mechanism,large language models(LLMs),prompt-engineering-based large language models (LLMs),human conversation patterns,a direct-answer-prediction process
"There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.",the figurative meaning understanding problem,inspiration,"Background: There has been little exploration of the capabilities of large vision-language models when dealing with figurative meaning in images and captions, such as metaphors or humor. This gap in research highlights the need for a better understanding of how these models can generalize from literal to figurative meaning, particularly when it is present in images.
Contribution: 'the figurative meaning understanding problem' inspired by ",1,an explainable visual entailment task,1-35768_0fe73cd2-ace7-4a20-a064-9b54f3e6b733,"['cs.cl', ' cs.ai', ' cs.cv']",a synthetic degradation pipeline,visual analogical reasoning in large multimodal models,fine-grained understanding of literal meaning,cognitive linguistics,a vision-language fusion problem
"Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.",operation logs and their time intervals for each student,inspiration,"Background: Previous research has primarily relied on statistical-based features derived from EventStream logs, which, while useful, do not capture the temporal information necessary to understand fine-grained differences in learning behaviors among students. This gap highlights the need for a more effective feature representation method that incorporates time information to enhance insights into student learning activities.
Contribution: 'operation logs and their time intervals for each student' inspired by ",1,a string sequence of characters,1-12405_0c94a078-64a1-47f0-bdda-1ddce0c2a94d,"['cs.cy', ' cs.ai', ' cs.cl', ' cs.lg']",hyperdimensional vector computing algebras,learning analytics,learning analytics,time-series analysis,a sequence of temporal graphs
"The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.",Large Language Models,inspiration,"Background: The auto-regressive decoding of Large Language Models (LLMs) results in significant overheads in hardware performance, particularly in terms of memory consumption and training cost, which are often overlooked in existing speculative decoding techniques. Additionally, there is a need to recover conditional dependency information for multi-token generation to improve acceptance rates for long-range predictions.
Contribution: 'Large Language Models' inspired by ",1,the human natural language generation process,1-38796_a6183606-5827-4750-892f-2505069e3051,"['cs.lg', ' cs.cl']",context has been well studied for learning representations,Decoder-only Large Language Models,decoder-only large language models (LLMs),recurrent neural networks,recent work that predicts the probabilities of subsequent tokens using multiple heads
"Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.",text generation,inspiration,"Background: Existing autoregressive decoding methods in large language models often lead to suboptimal token selection at chaotic points, which can significantly affect the quality of generated text. This highlights a need for improved decision-making processes during text generation to enhance overall model performance.
Contribution: 'text generation' inspired by ",1,the human decision-making process,1-89_7165d453-4adb-47d0-a27d-7544c6ab3133,"['cs.cl', ' cs.ai']",the feature selection process,Text generation in Large Language Models,controllable text generation approaches,reinforcement learning,a sequential decision-making task
"The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.",Post-training Quantization of Large Language Models (Large Language Models),inspiration,"Background: The current state-of-the-art approaches for Post-training Quantization (PTQ) often require calibration to achieve the desired accuracy, which can be a limitation in efficiently deploying Large Language Models (LLMs). Additionally, there is a need for methods that preserve privacy by eliminating the requirement for calibration or training data while maintaining model accuracy and information content.
Contribution: 'Post-training Quantization of Large Language Models (Large Language Models)' inspired by ",1,Adaptive LASSO regression model,1-29581_8487266a-eb35-4090-9193-90e818d727f7,['cs.cl'],an energy score,existing calibration methods for large language models,few-shot capabilities of large language models,zero-shot learning,the influence of label smoothing on model confidence and differential privacy
"The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.",patent prosecution,inspiration,"Background: The available human feedback in patent prosecution is limited, and the quality of generated patent text requires improvement. Additionally, there is a need for methods that can effectively utilize human feedback to enhance the usability of language models in this domain.
Contribution: 'patent prosecution' inspired by ",1,a system of reinforcement learning from human feedback,1-10365_8e7726ec-b651-4be4-b038-702e4bd761fe,['cs.cl'],image-based conditioning through cross-attention,the evaluation of generated text,human expert proofreading,active learning techniques,a system of reinforcement learning from human feedback
"Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.",giving deep networks a prior grounded in explicit medical knowledge communicated in natural language,inspiration,"Background: Deep networks often struggle with generalization when faced with domain shifts in medical imaging, such as variations in data from different hospitals or demographic factors. This highlights a gap in existing models, which lack appropriate architectural priors for reliable performance in these challenging scenarios.
Contribution: 'giving deep networks a prior grounded in explicit medical knowledge communicated in natural language' inspired by ",1,medical training,1-16109_761d4b7f-cf5a-4694-a15e-bace15daa624,"['cs.cv', ' cs.cl']",encoder-decoder model,enhance the capability of medical Multimodal Large Languange Models in understanding anatomical regions within entire medical scans,semantic high dimensional medical knowledge,clinical guidelines,"Inspired by this, we investigate the role of medical knowledge in disease diagnosis through doctor-patient interaction"
"Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.",enhance LLM creativity,inspiration,"Background: Large language models (LLMs) often struggle to generate creative and original responses to open-ended questions, indicating a need for improved methods to enhance their creativity. The challenge lies in overcoming the homogeneity of LLMs and fostering more diverse and vigorous idea exchanges to achieve creative outcomes.
Contribution: 'enhance LLM creativity' inspired by ",1,emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives,1-26574_2311f48f-a092-4c2d-9917-182c1e70ff5f,"['cs.cl', ' cs.ai']",the human review process,assessing the level of creativity in large language models,open language models,human creative processes,emulate the human process of inducing collective creativity through engaging discussions with participants from diverse backgrounds and perspectives
