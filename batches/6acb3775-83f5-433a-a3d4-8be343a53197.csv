context,anchor,relation,query,k,positive,id,random,mpnet_zero,sciIE,gpt-4o,ours
"Transformers, while highly performant in sequence modelling, are computationally expensive at inference time, which limits their applications in low-resource settings such as mobile and embedded devices. Additionally, traditional attention-based models cannot be updated efficiently with new tokens, which is a crucial requirement in sequence modelling.",attention,inspiration,"Background: Transformers, while highly performant in sequence modelling, are computationally expensive at inference time, which limits their applications in low-resource settings such as mobile and embedded devices. Additionally, traditional attention-based models cannot be updated efficiently with new tokens, which is a crucial requirement in sequence modelling.
Contribution: 'attention' inspired by ",1,a special Recurrent Neural Network with the ability to compute its many-to-one Recurrent Neural Network output efficiently,1-23808_9abe119d-1be9-4cea-ac0d-7aeb22a2abc4,formalize cross-domain interest transfer,the sequence-to-sequence model,hybrid Linear Attention/Transformer sequence model,sparse attention mechanisms,Transformers employ a similar notion of attention in their architecture
"Remote sensing image change captioning faces challenges in accurately localizing terrain changes due to pixel-level differences that arise from long time spans between image captures. This issue decreases the accuracy of generated captions, highlighting a need for improved methods that can effectively mitigate these impacts while generating fluent and relevant descriptions of semantic changes.",Remote sensing image change captioning,inspiration,"Background: Remote sensing image change captioning faces challenges in accurately localizing terrain changes due to pixel-level differences that arise from long time spans between image captures. This issue decreases the accuracy of generated captions, highlighting a need for improved methods that can effectively mitigate these impacts while generating fluent and relevant descriptions of semantic changes.
Contribution: 'Remote sensing image change captioning' inspired by ",1,the remarkable generative power of diffusion model,1-40574_b78a8efd-90b8-4993-b9aa-908b9d1de9d8,identify the events from accident reports,image difference captioning,semantic-level change captioning,attention mechanisms,image difference captioning
"The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.",Learned Visual Content Representation,inspiration,"Background: The design of existing Large Multimodal Models leads to inefficiencies due to an excessive number of tokens required for dense visual scenarios, such as high-resolution images and videos. Current token pruning and merging methods lack flexibility in balancing information density and efficiency, highlighting a need for improved representation strategies that can adapt to varying complexities of visual content.
Contribution: 'Learned Visual Content Representation' inspired by ",1,the concept of Matryoshka Dolls,1-28267_42e9dab5-5541-4427-8fac-63e27d0ea054,context-aware decoding,Large Multimodal Models,multimodal representations,biological vision systems,the token-mixing technique applied in 2D images
"The challenge of achieving a satisfactory balance between steganography quality and robustness against image disturbances has been a limitation in previous studies. Additionally, the specific requirements of QR Code steganography, which necessitate that the restored QR Code remains recognizable despite embedding into a natural image, further complicate the data concealing and revealing process.",Image steganography,inspiration,"Background: The challenge of achieving a satisfactory balance between steganography quality and robustness against image disturbances has been a limitation in previous studies. Additionally, the specific requirements of QR Code steganography, which necessitate that the restored QR Code remains recognizable despite embedding into a natural image, further complicate the data concealing and revealing process.
Contribution: 'Image steganography' inspired by ",2,the recent developments in transformer-based vision models,2-40202_ef575d55-4a4d-497d-b68b-d601ee3a89a0,a linear regression model,the fragility of video-into-video steganography,image steganography technology,neural networks,the fragility of video-into-video steganography
"Next-frame prediction serves as a foundational learning strategy to enhance the understanding of the visual world, addressing the need for models that can predict physical constants without explicit labeling. This approach demonstrates the potential to improve predictive capabilities significantly compared to random models, highlighting a gap in existing methods that require direct training for such tasks.",Next-frame prediction,inspiration,"Background: Next-frame prediction serves as a foundational learning strategy to enhance the understanding of the visual world, addressing the need for models that can predict physical constants without explicit labeling. This approach demonstrates the potential to improve predictive capabilities significantly compared to random models, highlighting a gap in existing methods that require direct training for such tasks.
Contribution: 'Next-frame prediction' inspired by ",1,the empirical success of causal language modelling and next-token prediction in language modelling,1-23115_d975b7bd-4318-49e7-a4a3-8e37fad6675e,intra-domain gradient matching,object prediction,object-level predictions,predictive coding theory,the remarkable success of the Feynman Technique in efficient human learning
"Large vision-language models (LVLMs) are unable to incorporate up-to-date knowledge due to the significant resources required for frequent updates, leading to failures in tasks that require current information. This limitation necessitates a solution that allows LVLMs to access and utilize recent knowledge during inference, particularly for applications like visual question answering (VQA).",internet-augmented generation,inspiration,"Background: Large vision-language models (LVLMs) are unable to incorporate up-to-date knowledge due to the significant resources required for frequent updates, leading to failures in tasks that require current information. This limitation necessitates a solution that allows LVLMs to access and utilize recent knowledge during inference, particularly for applications like visual question answering (VQA).
Contribution: 'internet-augmented generation' inspired by ",2,retrieval-augmented generation,2-1276_7bd24c93-71e9-4ce5-9702-4daa572cd118,standard neural networks,recent advancements in visual-language models,visual question answering task,retrieval-augmented generation,"the editing of knowledge sources,such as Multimodal LLMs"
"Existing approaches to active voltage control often overlook the constrained optimization nature of the problem, which can lead to failures in guaranteeing safety constraints. This highlights a need for methodologies that can effectively address these constraints while improving voltage quality and relieving power congestion in power networks.",Active voltage control,inspiration,"Background: Existing approaches to active voltage control often overlook the constrained optimization nature of the problem, which can lead to failures in guaranteeing safety constraints. This highlights a need for methodologies that can effectively address these constraints while improving voltage quality and relieving power congestion in power networks.
Contribution: 'Active voltage control' inspired by ",1,a constrained Markov game,1-19099_2f6b42be-b12e-4cd2-9c17-94fd98f02bc3,tutor talk moves,the Volt-VAR control problem,optimization-based active control pipeline,model predictive control,a constrained optimization problem
"The lack of widely acknowledged testing mechanisms for evaluating large language models (LLMs) has led to uncertainty regarding their comprehension capabilities, particularly in understanding nuanced semantics. Existing research primarily focuses on surface-level natural language understanding, neglecting the fine-grained explorations necessary for a deeper understanding of LLMs' unique comprehension mechanisms and their alignment with human cognition.",Large language models,inspiration,"Background: The lack of widely acknowledged testing mechanisms for evaluating large language models (LLMs) has led to uncertainty regarding their comprehension capabilities, particularly in understanding nuanced semantics. Existing research primarily focuses on surface-level natural language understanding, neglecting the fine-grained explorations necessary for a deeper understanding of LLMs' unique comprehension mechanisms and their alignment with human cognition.
Contribution: 'Large language models' inspired by ",1,foundational principles of human communication within psychology,1-18678_228afba2-62a8-47e6-95e9-955b0f596c9d,a dynamic observation module,"positioning Large Language Models as the digital counterpart to the Faculty of Verbal Knowledge, shedding light on their capacity to emulate certain facets of human reasoning",Large-Language model (LLM),cognitive neuroscience,human cognitive science
"Unsupervised domain adaptation for monocular depth estimation has been explored to reduce reliance on large annotated datasets, but this often requires training multiple models or complex protocols. The need for a simpler and more effective approach that can operate with only source domain ground truth labels is evident, as demonstrated by the challenges faced in prior work.",unsupervised domain adaptation for monocular depth estimation,inspiration,"Background: Unsupervised domain adaptation for monocular depth estimation has been explored to reduce reliance on large annotated datasets, but this often requires training multiple models or complex protocols. The need for a simpler and more effective approach that can operate with only source domain ground truth labels is evident, as demonstrated by the challenges faced in prior work.
Contribution: 'unsupervised domain adaptation for monocular depth estimation' inspired by ",2,a consistency-based semi-supervised learning problem,2-21426_67fc75b7-c12d-46c5-988a-a2dbf34a332a,the motif of one graph,Source-free Unsupervised Domain Adaptation,monocular depth estimation models,self-supervised learning,unsupervised domain adaptation
"Conventional approaches to Dynamic Scene Graph Generation often rely on multi-stage pipelines, which can lead to sub-optimal solutions due to the separation and independent optimization of various stages. Additionally, capturing temporal dependencies in videos presents a significant challenge, as existing methods may require additional trackers or handcrafted trajectories, limiting their effectiveness.",Dynamic Scene Graph Generation,inspiration,"Background: Conventional approaches to Dynamic Scene Graph Generation often rely on multi-stage pipelines, which can lead to sub-optimal solutions due to the separation and independent optimization of various stages. Additionally, capturing temporal dependencies in videos presents a significant challenge, as existing methods may require additional trackers or handcrafted trajectories, limiting their effectiveness.
Contribution: 'Dynamic Scene Graph Generation' inspired by ",2,a set prediction problem,2-7746_8761af15-b7c9-48f3-9201-5ce1d52e35ce,text embedding space,Scene Graph Generation models,scene graph generation,transformer models,a frames-to-frames generation task
